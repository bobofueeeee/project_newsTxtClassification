

# 构建vocab +  Embedding（嵌入）

### 1. Vocab（词汇表）

**定义**：

- Vocab是一个包含所有已知单词或符号的列表。在训练模型时，文本数据中的每个单词或符号都会被映射到这个列表中的一个索引上。

**构建过程**：

1. **数据预处理**：首先，从文本数据集中收集所有独特的单词或符号。
2. **过滤**：根据频率或其他标准（如最低词频阈值）来过滤单词，只保留最常用的单词。
3. **排序和索引**：将剩余的单词按照某种顺序（如字母顺序或频率顺序）排序，并为每个单词分配一个唯一的索引。

**示例**：

假设我们有一个简单的文本数据集，其中包含以下单词：“I”, “like”, “apples”, “bananas”, “and”, “oranges”。经过预处理和过滤后，我们得到以下的vocab：

![image-20240612180022896](https://gitee.com/fubob/note-pic/raw/master/image/image-20240612180022896.png)

### 2. Embedding（嵌入）

**定义**：

- Embedding是一种将单词或其他文本元素（如短语、句子等）转换为固定大小向量的技术。这些向量通常被称为嵌入向量或词嵌入，它们可以捕捉单词之间的语义和语法关系。

**构建过程**：

1. **初始化**：首先，为vocab中的每个单词创建一个空的嵌入向量。这些向量的大小（即嵌入维度）是预先设定的。
2. **训练**：在模型训练过程中，使用文本数据来更新这些嵌入向量的值。具体的训练方法取决于所使用的模型（如Word2Vec、GloVe等）。

以Word2Vec的Skip-gram模型为例，该模型的目标是根据给定的中心词来预测其上下文词。在训练过程中，模型的参数（包括嵌入向量）会不断被更新，以最小化预测错误。

**示例**（使用上面的vocab）：

假设我们设定嵌入维度为3，那么每个单词的嵌入向量就是一个长度为3的浮点数数组。在初始化时，这些数组可能包含随机值。然后，在模型训练过程中，这些值会根据模型的目标函数（如交叉熵损失）进行更新。

例如，“apples”的初始嵌入向量可能是`[0.1, -0.2, 0.3]`，但经过训练后，这个向量可能会变为`[0.4, 0.1, -0.2]`，以更好地捕捉“apples”与其他单词之间的语义关系。

# Vocab到Embedding模型的过程

所以，总结来说，先把自然语言通过Vocab转换为数字，再由Embedding转换为向量，这样理解正确吗

是的，您的理解完全正确。在自然语言处理中，我们通常首先通过词汇表（Vocab）将自然语言文本中的单词或符号转换为数字索引（整数），这个过程通常被称为“分词”或“标记化”（tokenization）。然后，这些数字索引被用作嵌入层（Embedding layer）的输入，嵌入层将这些索引转换为固定大小的向量表示（即嵌入向量）。

这个过程的关键在于，嵌入向量能够捕捉到单词之间的语义和语法关系，这使得神经网络模型能够更好地理解文本数据。嵌入向量通常是通过在大规模语料库上训练得到的，例如使用Word2Vec、GloVe等算法。这些算法通过学习单词的上下文信息来生成嵌入向量，使得相似的单词在向量空间中的位置相近。

因此，通过Vocab将自然语言转换为数字索引，再通过Embedding将这些索引转换为向量表示，是自然语言处理中常见的预处理步骤，它们共同为后续的模型训练和应用提供了基础。