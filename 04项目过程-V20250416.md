# 1. 数据理解

## 1.1 下载数据

https://tianchi.aliyun.com/competition/entrance/531810/information



## 1.2 背景理解

业务背景，对新闻进行分类，总共14个类别

```
{'科技': 0, '股票': 1, '体育': 2, '娱乐': 3, '时政': 4, '社会': 5, '教育': 6, '财经': 7, '家居': 8, '游戏': 9, '房产': 10, '时尚': 11, '彩票': 12, '星座': 13}
```

同时，数据已经做了数值化的编码处理，**需要注意的是，他这里的数值转化，使用的是词汇表的形式，约定“你”映射到`57 44`，“好”映射到`66 56`**，例如：

| label |                             text                             |
| :---: | :----------------------------------------------------------: |
|   6   | 57 44 66 56 2 3 3 37 5 41 9 57 44 47 45 33 13 63 58 31 17 47 0 1 1 69 26 60 62 15 21 12 49 18 38 20 50 23 57 44 45 33 25 28 47 22 52 35 30 14 24 69 54 7 48 19 11 51 16 43 26 34 53 27 64 8 4 42 36 46 65 69 29 39 15 37 57 44 45 33 69 54 7 25 40 35 30 66 56 47 55 69 61 10 60 42 36 46 65 37 5 41 32 67 6 59 47 0 1 1 68 |



需要注意的是，训练数据有800多M，较大，execl打开就很慢

评判的标准：F1 score

![image-20250416190942835](https://gitee.com/fubob/note-pic/raw/master/image/image-20250416190942835.png)



# 2. 数据预览

![image-20250416192042855](https://gitee.com/fubob/note-pic/raw/master/image/image-20250416192042855.png)



可以看到数据量20W，较大，需要注意

# 3. 算法准备

## 3.1 3种不同的向量方式

特征工程主要是对text进行向量化，如果它提供的是文本，还需要通过词汇表的形式，先转换成类似text的数值

向量话多种方式：

- 转换成词向量，主要用于统计词语频率

  - 使用于TF-IDF，词语频率 + 逆文档频率

  ```
  TF(t)= 该词语在当前文档出现的次数 / 当前文档中词语的总数
  IDF(t)= log_e（文档总数 / 出现该词语的文档总数）
  ```

  - 转换成CountVectorizer

- Embedding模型，可以主要就是向量化模型，这个在rag当作经常用到，常见的有bge,bgce,xiaobu等等

```
TF-IDF、CountVectorizer 和 Embedding 是三种常用的文本表示方法

1. TF-IDF（Term Frequency-Inverse Document Frequency）
TF-IDF 是一种统计方法，用于评估一个词在文档集合或语料库中的重要性。
TF（Term Frequency）：词频，表示一个词在文档中出现的频率。
IDF（Inverse Document Frequency）：逆文档频率，表示一个词在整个语料库中的稀有程度。

2. CountVectorizer
CountVectorizer 是一种词袋模型（Bag of Words, BoW）的实现，用于将文本转换为词频向量。
它会统计每个词在文档中出现的次数，并生成一个稀疏矩阵表示。

3. Embedding（词嵌入）
Embedding 是一种将词或句子映射到低维稠密向量的技术。
常见的词嵌入方法包括 Word2Vec、GloVe、FastText 和 BERT 等。
这些方法能够捕捉词与词之间的语义关系，如同义词、反义词等。
```

| **特性**       | **TF-IDF**         | **CountVectorizer** | **Embedding**         |
| -------------- | ------------------ | ------------------- | --------------------- |
| **表示方式**   | 稀疏向量           | 稀疏向量            | 稠密向量              |
| **语义信息**   | 无                 | 无                  | 有                    |
| **词序信息**   | 无                 | 无                  | 部分模型（如 BERT）有 |
| **维度**       | 高（词汇表大小）   | 高（词汇表大小）    | 低（通常 100-300 维） |
| **计算复杂度** | 低                 | 低                  | 高                    |
| **适用场景**   | 文本分类、信息检索 | 简单文本分类、聚类  | 复杂 NLP 任务         |

```
如何选择？
如果任务简单且计算资源有限：

选择 TF-IDF 或 CountVectorizer。
例如，文本分类、关键词提取等任务。

如果需要捕捉语义信息：
选择 Embedding。
例如，情感分析、机器翻译等任务。
如果需要处理大规模数据集或复杂任务：
选择预训练的 Embedding 模型（如 BERT）。
这些模型能够提供更好的语义表示和泛化能力。
```

```
示例
假设我们有一个简单的句子：“我喜欢自然语言处理”。

TF-IDF：
将句子转换为向量，表示每个词的重要性。
例如，[0.5, 0.3, 0.8]（假设词汇表为[“我”, “喜欢”, “自然语言处理”]）。

CountVectorizer：
将句子转换为词频向量。
例如，[1, 1, 1]（表示每个词出现一次）。

Embedding：
将句子中的每个词转换为稠密向量。
例如，使用 Word2Vec，“我”可能表示为 [0.1, 0.2, 0.3]，“喜欢”表示为 [0.4, 0.5, 0.6]，然后取平均或拼接得到句子向量。
```

### **TF-IDF、CountVectorizer 和 Embedding的区别**

- TF-IDF 和 CountVectorizer 适用于简单任务，计算效率高，但无法捕捉语义信息。
- Embedding 适用于复杂任务，能够捕捉语义信息，但需要更多的计算资源和数据。
- 在实际应用中，可以根据任务需求、计算资源和数据规模选择合适的文本表示方法。



## 3.2 5种不同的分类器

- RidgeClassifier岭回归
- RNN
- CNN
- LSTM
- Bert

```
1. RidgeClassifier（岭回归分类器）

概念
RidgeClassifier 是一种线性分类器，基于岭回归（Ridge Regression）的思想。
岭回归是一种线性回归的变体，通过引入 L2 正则化项来防止过拟合。

优点：简单高效，计算速度快。适用于高维数据，能够处理多重共线性。
缺点：只能处理线性可分或近似线性可分的问题。无法捕捉非线性关系。

应用场景:适用于特征数量较多且数据线性可分或近似线性可分的分类任务。

2. RNN（循环神经网络）
概念
RNN 是一种用于处理序列数据的神经网络，能够捕捉序列中的时间依赖性。
它通过循环结构将前一个时间步的隐藏状态传递到下一个时间步。

优点：能够处理变长序列数据。适用于自然语言处理、时间序列预测等任务。
缺点：容易出现梯度消失或梯度爆炸问题，导致长序列训练困难。计算效率较低，难以并行化。

应用场景: 自然语言处理（如文本生成、机器翻译）、时间序列分析、语音识别等。

3. CNN（卷积神经网络）
概念
CNN 是一种专门用于处理网格结构数据（如图像、音频）的神经网络。
它通过卷积层、池化层和全连接层提取数据的局部特征。

优点：能够自动提取数据的局部特征，减少人工特征工程。对平移、缩放、旋转等变换具有一定的鲁棒性。
缺点：主要适用于网格结构数据，对序列数据的处理能力有限。池化操作可能导致信息丢失。

应用场景: 图像处理（如图像分类、目标检测）、音频处理、视频分析等。


4. LSTM（长短期记忆网络）
概念
LSTM 是一种特殊的 RNN，通过引入门控机制（输入门、遗忘门、输出门）来解决梯度消失或梯度爆炸问题。
它能够更好地捕捉长序列中的长期依赖关系。

优点：能够处理长序列数据，避免梯度消失或梯度爆炸。适用于需要长期记忆的任务。
缺点：结构复杂，训练时间较长。

应用场景: 自然语言处理（如文本生成、情感分析）、时间序列预测、语音识别等。

5. BERT（Bidirectional Encoder Representations from Transformers）
概念
BERT 是一种基于 Transformer 架构的预训练语言模型。
它通过双向编码器学习文本的上下文表示，能够捕捉词与词之间的复杂关系。

优点：能够生成高质量的上下文表示，适用于多种 NLP 任务。预训练模型可以在大规模数据上学习通用语言知识，减少下游任务的训练时间和数据需求。
缺点：模型庞大，计算资源消耗巨大。需要大量的预训练数据和计算资源。

应用场景: 自然语言处理（如文本分类、命名实体识别、问答系统、机器翻译等）。
```

### **对比总结**

| **特性**       | **RidgeClassifier** | **RNN**                | **CNN**            | **LSTM**               | **BERT**                 |
| -------------- | ------------------- | ---------------------- | ------------------ | ---------------------- | ------------------------ |
| **类型**       | 线性分类器          | 序列模型               | 网格数据模型       | 序列模型               | 预训练语言模型           |
| **输入数据**   | 特征向量            | 序列数据               | 网格数据（如图像） | 序列数据               | 文本数据                 |
| **非线性能力** | 无                  | 有                     | 有                 | 有                     | 有                       |
| **长期依赖**   | 无                  | 有限                   | 无                 | 强                     | 强                       |
| **计算复杂度** | 低                  | 中等                   | 中等               | 高                     | 非常高                   |
| **应用场景**   | 线性分类任务        | 自然语言处理、时间序列 | 图像处理、音频处理 | 自然语言处理、时间序列 | 自然语言处理（多种任务） |



### **如何选择？**

- 如果数据线性可分或近似线性可分：
  - 选择 RidgeClassifier。
  - 例如，简单的二分类任务。
- 如果处理序列数据且需要捕捉时间依赖性：
  - 选择 RNN 或 LSTM。
  - 如果序列较长或需要长期记忆，优先选择 LSTM。
- 如果处理网格结构数据（如图像）：
  - 选择 CNN。
  - 例如，图像分类、目标检测等任务。
- 如果处理自然语言处理任务且需要高质量的上下文表示：
  - 选择 BERT 或其他预训练语言模型。
  - 例如，文本分类、命名实体识别、问答系统等任务。



### **示例**

- RidgeClassifier：
  - 适用于简单的文本分类任务，如垃圾邮件检测（假设特征已提取为向量）。
- RNN：
  - 适用于简单的文本生成任务，如字符级语言模型。
- CNN：
  - 适用于图像分类任务，如识别手写数字（MNIST 数据集）。
- LSTM：
  - 适用于情感分析任务，如分析电影评论的情感倾向。
- BERT：
  - 适用于复杂的自然语言处理任务，如问答系统、文本摘要等。



### **结论**

- RidgeClassifier 适用于线性分类任务，计算效率高。
- RNN 和 LSTM 适用于序列数据处理，LSTM 更适合长序列。
- CNN 适用于网格结构数据，如图像和音频。
- BERT 是一种强大的预训练语言模型，适用于多种自然语言处理任务。
- 在实际应用中，应根据任务需求、数据类型和计算资源选择合适的模型。



# 4. AI工程

## 4.1 CountVectorizer + RidgeClassifier

```python
train = pd.read_csv('/root/data/train_set.csv',sep='\t') 

# 1. 使用CountVectorizer，转换词向量
countvector = CountVectorizer(max_features=3000)
train_vec = countvector.fit_transform(train['text'])

# 2. 模型训练
clf = RidgeClassifier()
clf.fit(train_vec,train['label'])

# 3. 模型评估
envalue_pred = clf.predict(train_vec[:10000])
envalue_f1_score = f1_score(train['label'][:10000], envalue_pred, average='macro')
envalue_f1_score

# 4. 模型预测
test = pd.read_csv('/root/data/train_set.csv/train_set.csv', sep = '\t')
pred = clf.predict(test)
test['label'] = pred
test['label'].to_csv("/root/data/countVectorRidgeclassification.csv", index=False)  
```



## 4.2 TF-IDF + RidgeClassifier

```python
train = pd.read_csv('/root/data/train_set.csv',sep='\t') 

# 1. 使用TfidfVectorizer，转换成词向量
tfidfvector = TfidfVectorizer(max_features=3000)
train_vec = tfidfvector.fit_transform(train['text'])

# 2. 模型训练
clf = RidgeClassifier()
clf.fit(train_vec,train['label'])

# 3. 模型评估
envalue_pred = clf.predict(train_vec[:10000])
envalue_f1_score = f1_score(train['label'][:10000], envalue_pred, average='macro')
envalue_f1_score

# 4. 模型预测
test = pd.read_csv('/root/data/train_set.csv/train_set.csv', sep = '\t')
pred = clf.predict(test)
test['label'] = pred
test['label'].to_csv("/root/data/countVectorRidgeclassification.csv", index=False)  
```



## 4.3 RidgeClassifier分类模型存储

```python
import joblib

joblib.dump(clf, 'ridge_classifier_model.joblib')
# 从文件加载模型
loaded_clf = joblib.load('ridge_classifier_model.joblib')
 
# 使用加载的模型进行预测（示例）
predictions = loaded_clf.predict(train_vec[:5])  # 假设使用前 5 个样本进行预测
print(predictions)
```



```python
import pickle

# 训练 RidgeClassifier 模型（与上面相同）
clf = RidgeClassifier()
clf.fit(train_vec, train_label)

# 保存模型到文件
with open('ridge_classifier_model.pkl', 'wb') as file:
    pickle.dump(clf, file)
```

```python
# 从文件加载模型
with open('ridge_classifier_model.pkl', 'rb') as file:
    loaded_clf = pickle.load(file)

# 使用加载的模型进行预测（示例）
predictions = loaded_clf.predict(train_vec[:5])  # 假设使用前 5 个样本进行预测
print(predictions)
```



## 4.4 embedding + 分类器

### 4.3.1 选择embedding模型

先选取一个好的embedding模型，将训练的label标签，转换成高纬度的稠密矩阵

```
1. Word2Vec
概念
Word2Vec 是一种常用的词嵌入方法，可以将离散的词映射到连续的向量空间中。
它通过训练神经网络来学习词的向量表示，捕捉词与词之间的语义关系。

优点： 简单易用，计算效率高。能够捕捉词与词之间的相似性。
缺点： 适用于文本数据，对于非文本标签可能需要自定义词汇表。无法处理未登录词（out-of-vocabulary words）。

应用场景： 适用于将离散的标签或词汇转换为稠密向量。

2. GloVe（Global Vectors for Word Representation）
概念
GloVe 是一种基于词共现矩阵的词嵌入方法。
它通过统计词与词在语料库中的共现次数来学习词的向量表示。

优点：能够捕捉全局的词共现信息。向量表示通常具有较好的语义解释性。
缺点：依赖于大规模的语料库进行训练。对于小数据集或特定领域的标签，可能效果不佳。

应用场景：适用于需要全局语义信息的词嵌入任务。

3. FastText
FastText 是 Facebook 开发的词嵌入方法，扩展了 Word2Vec。
它通过考虑词的子词（subword）信息来学习词的向量表示，能够更好地处理未登录词。

优点：能够处理未登录词，适用于小数据集或特定领域的标签。训练速度快，内存效率高。
缺点：对于非常短的标签或词汇，子词信息可能有限。
应用场景： 适用于需要处理未登录词或特定领域标签的嵌入任务。


4. BERT（Bidirectional Encoder Representations from Transformers）
概念
BERT 是一种基于 Transformer 架构的预训练语言模型。
它通过双向编码器学习文本的上下文表示，能够捕捉词与词之间的复杂关系。

优点：能够生成高质量的上下文表示，适用于多种 NLP 任务。预训练模型可以在大规模数据上学习通用语言知识。
缺点：模型庞大，计算资源消耗巨大。需要大量的预训练数据和计算资源。

应用场景：适用于需要高质量上下文表示的 NLP 任务，但可能过于复杂用于简单的标签嵌入。

5. 自定义嵌入层（如 Keras Embedding Layer）
在深度学习中，可以使用自定义的嵌入层将离散的标签映射到稠密向量。
嵌入层通常作为神经网络的第一层，将整数编码的标签转换为稠密向量。

优点：灵活，可以根据任务需求自定义嵌入维度。适用于各种类型的离散标签，不仅限于文本。
缺点：需要足够的训练数据来学习有意义的嵌入。嵌入向量是随机初始化的，可能需要更多的训练时间。

应用场景：适用于深度学习模型，特别是当标签是离散且数量有限时。
```

这里更适合于自定义嵌入层（如 Keras Embedding Layer），以下是一个简单的示例，使用tensorflow库：

```python
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Flatten, Dense

# 假设标签序列和对应的整数编码
labels = np.array([57, 44, 66, 56, 2, 3, 3, 37, 5, 41, 9, 57, 44, 47, 45, 33, 13, 63, 58, 31, 17, 47, 0, 1, 1, 69, 26, 60, 62, 15, 21, 12, 49, 18, 38, 20, 50, 23, 57, 44, 45, 33, 25, 28, 47, 22, 52, 35, 30, 14, 24, 69, 54, 7, 48, 19, 11, 51, 16, 43, 26, 34, 53, 27, 64, 8, 4, 42, 36, 46, 65, 69, 29, 39, 15, 37, 57, 44, 45, 33, 69, 54, 7, 25, 40, 35, 30, 66, 56, 47, 55, 69, 61, 10, 60, 42, 36, 46, 65, 37, 5, 41, 32, 67, 6, 59, 47, 0, 1, 1, 68])

# 标签的最大值，用于确定嵌入层的输入维度
num_labels = np.max(labels) + 1

# 嵌入维度，可以根据任务需求调整
embedding_dim = 10

# 构建模型
model = Sequential([
    Embedding(input_dim=num_labels, output_dim=embedding_dim, input_length=len(labels)),
    Flatten(),
    Dense(1, activation='sigmoid')  # 示例输出层，可以根据任务需求调整
])

# 编译模型
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# 打印模型摘要
model.summary()

# 示例：将标签序列转换为嵌入矩阵
embedding_layer = model.layers[0]
embedding_matrix = embedding_layer(labels)
print(embedding_matrix.shape)  # 输出嵌入矩阵的形状
```

使用pytoch的示例：

```python
import torch
import torch.nn as nn

# 标签序列（假设已经是整数编码）
text = [57, 44, 66, 56, 2, 3, 3, 37, 5, 41, 9, 57, 44, 47, 45, 33, 13, 63, 58, 31, 17, 47, 0, 1, 1, 69, 26, 60, 62, 15, 21, 12, 49, 18, 38, 20, 50, 23, 57, 44, 45, 33, 25, 28, 47, 22, 52, 35, 30, 14, 24, 69, 54, 7, 48, 19, 11, 51, 16, 43, 26, 34, 53, 27, 64, 8, 4, 42, 36, 46, 65, 69, 29, 39, 15, 37, 57, 44, 45, 33, 69, 54, 7, 25, 40, 35, 30, 66, 56, 47, 55, 69, 61, 10, 60, 42, 36, 46, 65, 37, 5, 41, 32, 67, 6, 59, 47, 0, 1, 1, 68]

# 将标签序列转换为张量
text_tensor = torch.tensor(text, dtype=torch.long)

# 词汇表大小（标签的最大值 + 1）
vocab_size = 10000

# 嵌入向量的维度
embedding_dim = 128

# 定义嵌入层
embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)

# 获取嵌入向量
embedded_vectors = embedding_layer(text_tensor)

print("Embedded vectors shape:", embedded_vectors.shape)
print("Embedded vectors:", embedded_vectors)
```

### 4.3.2 embedding工程化

```python
import torch
import torch.nn as nn
import pandas as pd

train = pd.read_csv('/root/data/train_set.csv',sep='\t') 

# 词汇表大小（标签的最大值 + 1）
vocab_size = max(train['label'][0]) + 1
# 嵌入向量的维度
embedding_dim = 100
# 定义嵌入层
embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)
embedded_vectors = []

for label in train['label']:
    labels_tensor = torch.tensor(label, dtype=torch.long)
	embedded_vector = embedding_layer(labels_tensor)
    embedded_vectors.append(embedded_vector)
    
embedded_vectors  
```

### 4.3.3 分类器工程化

#### RidgeClassifier

```python
# 2. 模型训练
clf = RidgeClassifier()
clf.fit(embedded_vectors,train['label'])

# 3. 模型评估
envalue_pred = clf.predict(train_vec[:10000])
envalue_f1_score = f1_score(train['label'][:10000], envalue_pred, average='macro')
envalue_f1_score

# 4. 模型预测
test = pd.read_csv('/root/data/train_set.csv/train_set.csv', sep = '\t')
pred = clf.predict(test)
test['label'] = pred
test['label'].to_csv("/root/data/countVectorRidgeclassification.csv", index=False)  
```

#### LSTM

为了使用 LSTM 对标签序列的嵌入向量进行多分类训练，你需要完成以下几个步骤：

1. **准备数据**：将标签序列转换为嵌入向量，并准备多分类标签。
2. **定义模型**：构建包含嵌入层、LSTM 层和全连接层的模型。
3. **训练模型**：使用合适的损失函数和优化器进行训练。

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import pandas as pd

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
train = pd.read_csv('/root/data/train_set.csv',sep='\t') 


# 1. 数据预处理，转换成嵌入层能够处理的格式
# 1.1 text转换成张量
text = train['text'].tolist()
def process_text(text, target_length=1057):
    numbers = list(map(int, text.split()))
    if len(numbers) < target_length:
        numbers.extend([0] * (target_length - len(numbers)))  # 填充0
    elif len(numbers) > target_length:
        numbers = numbers[:target_length]  # 截断
    return numbers

text_numbers = [process_text(item) for item in text]
text_tensor = torch.tensor(text_numbers, dtype=torch.long)
text_tensor.to(device)

## 1.2 转化成dataloader
class LabelDataset(Dataset):
    def __init__(self, label_sequences, multi_class_labels):
        self.label_sequences = [torch.tensor(seq, dtype=torch.long) for seq in label_sequences]
        self.multi_class_labels = multi_class_labels
 
    def __len__(self):
        return len(self.label_sequences)
 
    def __getitem__(self, idx):
        return self.label_sequences[idx], self.multi_class_labels[idx]
    
# 创建数据集和数据加载器
dataset = LabelDataset(text_tensor, train['label'])
dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

# 2. 定义模型
class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):
        super(LSTMClassifier, self).__init__()
        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, num_classes)

    def forward(self, x):
        embedded = self.embedding(x)  # 嵌入层
        lstm_out, _ = self.lstm(embedded)  # LSTM 层
        # 取 LSTM 的最后一个时间步的输出
        last_output = lstm_out[:, -1, :]
        output = self.fc(last_output)  # 全连接层
        return output

# 参数设置
vocab_size = 10000
embedding_dim = 128
hidden_dim = 20
num_classes = 14

# 实例化模型
model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, num_classes)
model.to(device)

# 3. 模型训练
# 损失函数和优化器
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 训练循环
num_epochs = 5
for epoch in range(num_epochs):
    for label_seq, label in dataloader:
        # 前向传播
        outputs = model(label_seq.to(device))
        loss = criterion(outputs, label.to(device))

        # 反向传播和优化
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')
    
    
# 4. 模型存储与加载
## 4.1 模型存储
torch.save(model.state_dict(), 'lstm_classifier.pth')

## 4.2 模型加载
# 定义模型结构
model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, num_classes)
# 加载模型状态字典
model.load_state_dict(torch.load('lstm_classifier.pth'))
# 将模型移动到 GPU（如果可用）
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
    

# 5. 模型预测
test = pd.read_csv('/root/data/train_set.csv/train_set.csv', sep = '\t')

## 5.1 对test数据集进行预处理
test_text = test['text'].tolist()
test_numbers = [process_text(item) for item in test_text]
test_tensor = torch.tensor(test_numbers, dtype=torch.long)
dataloader = DataLoader(test_tensor, batch_size=1, shuffle=True)

preds = []
with torch.no_grad():
    for batch in dataloader:
        outputs = model(batch.to(device))
        prob = torch.nn.functional.softmax(outputs, dim=1)
        predicted_classes = prob.argmax(dim=1)
        pres.append(predicted_classes.cpu().tolist())
## 5.2. 结果保存
test['label'] = preds
test['label'].to_csv("/root/data/countVectorRidgeclassification.csv", index=False)  
```

![image-20250417175038215](https://gitee.com/fubob/note-pic/raw/master/image/image-20250417175038215.png)

![image-20250417175050536](https://gitee.com/fubob/note-pic/raw/master/image/image-20250417175050536.png)

![image-20250417175101285](https://gitee.com/fubob/note-pic/raw/master/image/image-20250417175101285.png)

![image-20250417175114678](https://gitee.com/fubob/note-pic/raw/master/image/image-20250417175114678.png)



#### BERT

- 模型训练

```python
# 1. 记载数据
df_train = pd.read_csv('/root/data/train_set.csv', sep='\t')
```

```python
# 2. 数据预处理
## 2.1 数据通过bert模型进行向量化
train_dataset = Dataset.from_pandas(df_train)
train_dataset['label'][0]
tokenizer = BertTokenizer.from_pretrained('/root/model/bert-base-chinese')
def preprocess_function(examples):
    return tokenizer(examples['text'], truncation=True, padding=True, max_length=128)
encoded_dataset = train_dataset.map(preprocess_function, batched=True)
train_dataset_torch = encoded_dataset.with_format('torch')
train_loader = DataLoader(train_dataset_torch, batch_size=16, shuffle=True)
```

```python
# 3. 加载模型
num_labels = len(set(df_train['label']))  # 假设标签是从 0 开始的连续整数
num_labels
model = BertForSequenceClassification.from_pretrained('/root/model/bert-base-chinese', num_labels=num_labels)
# 优化器
optimizer = AdamW(model.parameters(), lr=3e-5)
num_training_steps = len(train_loader) * 3  # 假设训练 3 个 epoch
lr_scheduler = get_scheduler("linear", optimizer=optimizer, num_warmup_steps=num_training_steps * 0.1, num_training_steps=num_training_steps)
# 设备配置
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
```

```python
# 4. 训练模型
model.train()
for epoch in range(3):  # 训练 3 个 epoch
    for i , batch in enumerate(train_loader):
        # print(batch)  'target':batch['label'].to(device), 
        outputs = model(**{'input_ids':batch['input_ids'].to(device),'token_type_ids':batch['token_type_ids'].to(device),'attention_mask':batch['attention_mask'].to(device)})
        # print(outputs)
        logits = outputs.logits

        # 如果需要计算损失（在训练时）
        loss_fn = torch.nn.CrossEntropyLoss()

        loss = loss_fn(logits, batch['label'].to(device))
         
        # print("Logits:", logits)
        if i %10 == 0: 
            print("Loss:", loss.item()) 
        
        loss.backward()
 
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
    print("Loss:", loss.item()) 
    print(f"Epoch {epoch + 1} completed.")
```

```python
# 5. 模型存储
torch.save(model.state_dict(), 'BertForSequenceClassification.pth')
```



- 模型评估

```python
# 1. 加载测试集
df_test = pd.read_csv('/root/data/test_a.csv', sep='\t')
# 2. 数据预处理
test_dataset = Dataset.from_pandas(df_test)
tokenizer = BertTokenizer.from_pretrained('/root/model/bert-base-chinese')
def preprocess_function(examples):
    return tokenizer(examples['text'], truncation=True, padding=True, max_length=128)
encoded_dataset = test_dataset.map(preprocess_function, batched=True)
test_dataset_torch = encoded_dataset.with_format('torch')
test_loader = DataLoader(test_dataset_torch, batch_size=16)
```

```python
# 3. 加载模型
# 3.1  初始化模型（架构必须与保存时相同）
model = BertForSequenceClassification.from_pretrained('/root/model/bert-base-chinese', num_labels=14)  # 假设有2个分类标签
# 3.2 加载状态字典
model.load_state_dict(torch.load('BertForSequenceClassification.pth'))
 # 3.3 将模型设置为评估模式（如果需要）
model.eval()
model.to(device)
```

```python
# 4. 测试集评估
all_preds = []
with torch.no_grad():
    for batch in test_loader:
        # print(batch)
        input_ids = batch['input_ids'].to(device)
        token_type_ids = batch['token_type_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
    
         # 前向传播获取预测结果
        outputs = model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)
        logits = outputs.logits
        preds = torch.argmax(logits, dim=1).cpu().numpy()
        # print(preds)
        all_preds.extend(preds)
```

```python
# 5. 结果存储
df_test['label'] = all_preds
df_test['label'].to_csv("/root/data/BertForSequenceClassification .csv", index=False) 
```



1. **将模型设置为评估模式**：使用 `model.eval()`，这会关闭 dropout 和 batch normalization 等在训练时启用的层。
2. argmax  和 softmax的区别
   - **功能**：`argmax` 是一种操作，用于找到向量中最大值的索引。它返回具有最大值的元素的索引，而不是值本身。
   - **功能**：`softmax` 是一种激活函数，通常用于多分类问题的输出层。它将一个向量（通常是模型的原始输出，称为 logits）转换为概率分布。







# 5. 报错⭐⭐

- ![image-20250417171226142](https://gitee.com/fubob/note-pic/raw/master/image/image-20250417171226142.png)

  - 需要将原本的字符串转换为torch张量

  - ```python
    def process_text(text, target_length=1057):
        numbers = list(map(int, text.split()))
        if len(numbers) < target_length:
            numbers.extend([0] * (target_length - len(numbers)))  # 填充0
        elif len(numbers) > target_length:
            numbers = numbers[:target_length]  # 截断
        return numbers
    # 处理所有行数据
    processed_data = [process_text(item) for item in text_column]
     
    # 将列表转换为张量
    text_tensor = torch.tensor(processed_data, dtype=torch.long)
    ```

    

- 转换张量时：ValueError: expected sequence of length 1057 at dim 1 (got 486)

  - ```python
    加入0的填充
    def process_text(text, target_length=1057):
        numbers = list(map(int, text.split()))
        if len(numbers) < target_length:
            numbers.extend([0] * (target_length - len(numbers)))  # 填充0
        elif len(numbers) > target_length:
            numbers = numbers[:target_length]  # 截断
        return numbers
    # 处理所有行数据
    processed_data = [process_text(item) for item in text_column]
     
    # 将列表转换为张量
    text_tensor = torch.tensor(processed_data, dtype=torch.long)
    ```



- RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)

  - 将张量和嵌入层，都打到gpu上

  - ```
    embedding_layer.to(device)
    embedded_vectors = embedding_layer(text_tensor.to(device))
    ```



- CUDA out of memory. Tried to allocate 100.80 GiB. GPU 0 has a total capacity of 31.50 GiB of which 28.08 GiB is free. 
  - 这里为什么要100.80 GiB，明显不对，显然是代码有问题

  - 一次处理可能需要这么多，所以需要分批次处理，一次处理20G的数据量，embedding的时候也需要分批次吗？需要，需要通过dataloader，对数据进行批次的划分

  - ```python
    ## 1.2 转化成dataloader
    class LabelDataset(Dataset):
        def __init__(self, label_sequences, multi_class_labels):
            self.label_sequences = [torch.tensor(seq, dtype=torch.long) for seq in label_sequences]
            self.multi_class_labels = multi_class_labels
     
        def __len__(self):
            return len(self.label_sequences)
     
        def __getitem__(self, idx):
            return self.label_sequences[idx], self.multi_class_labels[idx]
            
    # 创建数据集和数据加载器
    dataset = LabelDataset(text_tensor, train['label'])
    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)
    ```

    

- ![image-20250417171807725](https://gitee.com/fubob/note-pic/raw/master/image/image-20250417171807725.png)

  - 实际类别数超过了设置的类别数，修改类别数量，将num_classes=3改为num_classes=14

  - ```python
    num_classes = 14
    
    # 实例化模型
    model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, num_classes)
    ```



- ![image-20250417173742729](https://gitee.com/fubob/note-pic/raw/master/image/image-20250417173742729.png)

  - 再次出现这种报错

  - 需要训练过程中的数据，都打到GPU上

  - ```python
    # 训练循环
    num_epochs = 5
    for epoch in range(num_epochs):
        for label_seq, label in dataloader:
            # 前向传播
            outputs = model(label_seq.to(device))
            loss = criterion(outputs, label.to(device))
    
            # 反向传播和优化
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
    
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')
    ```

  - 通过观察训练过程中的GPU使用情况，来判断是否打到了GPU上

  - ![](https://gitee.com/fubob/note-pic/raw/master/image/image-20250417173926908.png)



- 模型预测阶段

- ![image-20250417184336044](https://gitee.com/fubob/note-pic/raw/master/image/image-20250417184336044.png)

  - 对text数据集，也要进行dataloader的批次划分

  - ```PYTHON
    ## 5.2 记载到dotaloader里面
    # 创建数据集和数据加载器
    # test_dataset = LabelDataset(test_tensor, test['label'])
    test_dataloader = DataLoader(test_tensor, batch_size=3, shuffle=True)
    
    ## 5.3 对结果进行预测
    # 存储所有批次的预测结果
    all_predictions = []
     
    # 禁用梯度计算，减少内存消耗
    with torch.no_grad():
        for batch in test_dataloader:
            # 将批次数据移动到与模型相同的设备
            batch = batch.to(device)
     
            # 执行前向传播
            outputs = model(batch)
     
            # 假设模型输出的是对数概率，使用 softmax 转换为概率
            probs = torch.nn.functional.softmax(outputs, dim=1)
     
            # 获取预测类别（具有最高概率的类别）
            predicted_classes = probs.argmax(dim=1)
     
            # 收集预测结果
            all_predictions.extend(predicted_classes.cpu().tolist())
    ```






- 训练bert分类模型时，模型无法收敛

  - ![image-20250421201501017](https://gitee.com/fubob/note-pic/raw/master/image/image-20250421201501017.png)

  - 原因

    - 学习率问题：学习率过高可能导致模型在损失曲面上跳跃，无法收敛；学习率过低则可能导致收敛速度极慢。
      - 使用学习率调度器（如 `StepLR`, `ReduceLROnPlateau`）动态调整学习率
      - 使用学习率预热（warmup）策略。
    - 数据问题：数据质量差、数据预处理不当、数据不平衡等
      - 检查数据质量，确保数据没有错误或异常值。
      - 对数据进行标准化或归一化处理。
      - 如果数据不平衡，尝试使用过采样、欠采样或数据增强技术。
    - 模型架构问题：模型可能过于简单，无法捕捉数据中的复杂模式；或者模型过于复杂，导致过拟合。
      - 尝试使用更复杂的模型架构，或者增加模型的层数和参数。
      - 如果模型过拟合，考虑使用正则化技术（如 L2 正则化、dropout）或简化模型。
    - 优化器问题：优化器可能不适合当前任务，或者优化器的参数设置不当。
      - 尝试使用不同的优化器（如 SGD, Adam, RMSprop）。
      - 调整优化器的参数，如动量、权重衰减等。
    - 梯度问题：梯度消失或梯度爆炸可能导致模型无法有效学习。
      - 使用梯度裁剪（gradient clipping）来防止梯度爆炸。
      - 使用更稳定的激活函数（如 ReLU 替代 sigmoid 或 tanh）来缓解梯度消失。
      - 检查模型架构，确保梯度能够顺畅地流动。
    - 初始化问题：模型参数的初始化可能不当，导致训练不稳定。
      - 使用合适的初始化方法（如 Xavier 初始化、He 初始化）。
    - 批量大小问题：批量大小可能过大或过小，影响训练的稳定性和速度。
      - 尝试调整批量大小，找到最适合当前任务和硬件配置的批量大小。
    - 训练论述不足：模型可能还没有足够的时间来学习数据中的模式。
      - 增加训练轮数，但注意监控过拟合现象。
    - 早停法：有时模型在验证集上的性能会开始下降，而训练集上的损失仍在下降，这表示模型开始过拟合。
      - 使用早停法，在验证集性能不再提升时停止训练。
    - 调试与监控：有时问题可能出在代码实现或训练过程中的某个环节。
      - 使用调试工具检查代码，确保没有错误。
      - 监控训练过程中的损失、准确率等指标，以及梯度、权重等参数的变化。

  - 解决办法1：在优化器中加入预热学习

    ```
    num_warmup_steps = int(num_training_steps * 0.1)
    ```

  - 增大学习率

    ```
    减小学习率：尝试 3e-5, 1e-5 等。
    增大学习率：尝试 1e-4, 3e-4 等，但注意不要过大，以免导致训练不稳定
    lr=1e-3 ， lr=3e-4， lr=5e-4 都无法正常收敛，loss一直在2点几徘徊
    最后 optimizer = AdamW(model.parameters(), lr=3e-5) 可以正常收敛
    ```

    ![image-20250421211542391](https://gitee.com/fubob/note-pic/raw/master/image/image-20250421211542391.png)













# 6. 总结⭐⭐

## 6.1 LSMT

### 6.1.1 训练过程

- 数据的预处理

  - 数据空值填充，以及超过预定长度的截断，保证所有数组的长度统一
  - 转换为为张量
  - dataloader的初始化，注意训练时，应为有[text,label]，所以需要通过dataset进行转换，再加载为dataloader
  - 转换为dataloader是为了划分批次，每个批次处理一部分数据，免得处理所有数据的话内存溢出，可以再内存充足的情况下，尽量保证少批次，保证数据的连续性

  ```python
  def process_text(text, target_length=1057):
      numbers = list(map(int, text.split()))
      if len(numbers) < target_length:
          numbers.extend([0] * (target_length - len(numbers)))  # 填充0
      elif len(numbers) > target_length:
          numbers = numbers[:target_length]  # 截断
      return numbers
  text_numbers = [process_text(item) for item in text]
  text_tensor = torch.tensor(text_numbers, dtype=torch.long) 
  
  class LabelDataset(Dataset):
      def __init__(self, label_sequences, multi_class_labels):
          self.label_sequences = [torch.tensor(seq, dtype=torch.long) for seq in label_sequences]
          self.multi_class_labels = multi_class_labels
   
      def __len__(self):
          return len(self.label_sequences)
      
   	# 迭代器获取其中的每个元素
      def __getitem__(self, idx):
          return self.label_sequences[idx], self.multi_class_labels[idx]
  #
  dataset = LabelDataset(text_tensor, train['label'])
  dataloader = DataLoader(test01_tensor, batch_size=1, shuffle=True)
  ```

  

- 模型训练，张量和模型都需要.to(device)

  - ```PYTHON
    for epoch in range(num_epochs):
        for label_seq, label in dataloader:
            # 前向传播
            outputs = model(label_seq.to(device))
            loss = criterion(outputs, label.to(device))
    
            # 反向传播和优化
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
    
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')
    ```

### 6.1.2 预测过程

- 同样需要通过转换为DataLoader

```python
test = pd.read_csv('/root/data/train_set.csv/train_set.csv', sep = '\t')

## 5.1 对test数据集进行预处理
test_text = test['text'].tolist()
test_numbers = [process_text(item) for item in test_text]
test_tensor = torch.tensor(test_numbers, dtype=torch.long)
dataloader = DataLoader(test_tensor, batch_size=1)

preds = []
with torch.no_grad():
    for batch in dataloader:
        outputs = model(batch.to(device))
        prob = torch.nn.functional.softmax(outputs, dim=1)
        predicted_classes = prob.argmax(dim=1)
        pres.append(predicted_classes.cpu().tolist())
## 5.2. 结果保存
test['label'] = preds
test['label'].to_csv("/root/data/countVectorRidgeclassification.csv", index=False)  
```



### 6.1.3 总结

- 整体过程
  - 预处理，将数据转换成统一维度的，分类器能够识别的张量形式；不足的填充，超过的截断
  - 转换为dotaloader加载器，能够进行分批次的遍历，并附带shuffle
  - 模型训练；包括正向传播，计算loss，通过loss的反向传播进行梯度优化
  - 模型预测：将预测集转换为训练同样的格式，输入到模型中进行预测，注意再将输出的张量转换为概率值

- 为什么要进行DataLoader的转换
  - 如果一次处理全部数据，会显存溢出，所以需要分批次；
  - dataset：主要用过数据预处理
  - batch-size：每个批次处理的数据条数
  - 但在显存足够的情况下，尽量调大batch-size，保证数据连续性，特别时在LLM训练中



## 6.2 词频向量 + 分类器

### 6.2.1 总结

- 整体过程
  - 预处理，将数据转换为词频向量，一是countervec，二是tfidfvec，也可以是其他向量
  - 模型训练，通过fit_transfom方法，训练分类器，可以是RidgeClassifier，也可以lightgbm，也可以是其他分类器；训练过程中，通过交叉验证，观察f1_score，是否可以通过调整折数，或者其他参数进行优化
  - 模型预测，将测试集转换同样的词向量，输入模型进行预测





# 6. 结果评估

![image-20250422115156792](https://gitee.com/fubob/note-pic/raw/master/image/image-20250422115156792.png)

高分参考：https://github.com/MM-IR/rank4_NLP_textclassification/tree/master