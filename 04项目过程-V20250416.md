# 1. 数据理解

## 1.1 下载数据

https://tianchi.aliyun.com/competition/entrance/531810/information



## 1.2 背景理解

业务背景，对新闻进行分类，总共14个类别

```
{'科技': 0, '股票': 1, '体育': 2, '娱乐': 3, '时政': 4, '社会': 5, '教育': 6, '财经': 7, '家居': 8, '游戏': 9, '房产': 10, '时尚': 11, '彩票': 12, '星座': 13}
```

同时，数据已经做了数值化的编码处理，**需要注意的是，他这里的数值转化，使用的是词汇表的形式，约定“你”映射到`57 44`，“好”映射到`66 56`**，例如：

| label |                             text                             |
| :---: | :----------------------------------------------------------: |
|   6   | 57 44 66 56 2 3 3 37 5 41 9 57 44 47 45 33 13 63 58 31 17 47 0 1 1 69 26 60 62 15 21 12 49 18 38 20 50 23 57 44 45 33 25 28 47 22 52 35 30 14 24 69 54 7 48 19 11 51 16 43 26 34 53 27 64 8 4 42 36 46 65 69 29 39 15 37 57 44 45 33 69 54 7 25 40 35 30 66 56 47 55 69 61 10 60 42 36 46 65 37 5 41 32 67 6 59 47 0 1 1 68 |



需要注意的是，训练数据有800多M，较大，execl打开就很慢

评判的标准：F1 score

![image-20250416190942835](https://gitee.com/fubob/note-pic/raw/master/image/image-20250416190942835.png)



# 2. 数据预览

![image-20250416192042855](https://gitee.com/fubob/note-pic/raw/master/image/image-20250416192042855.png)



可以看到数据量20W，较大，需要注意

# 3. 算法准备

## 3.1 3种不同的向量方式

特征工程主要是对text进行向量化，如果它提供的是文本，还需要通过词汇表的形式，先转换成类似text的数值

向量话多种方式：

- 转换成词向量，主要用于统计词语频率

  - 使用于TF-IDF，词语频率 + 逆文档频率

  ```
  TF(t)= 该词语在当前文档出现的次数 / 当前文档中词语的总数
  IDF(t)= log_e（文档总数 / 出现该词语的文档总数）
  ```

  - 转换成CountVectorizer

- Embedding模型，可以主要就是向量化模型，这个在rag当作经常用到，常见的有bge,bgce,xiaobu等等

```
TF-IDF、CountVectorizer 和 Embedding 是三种常用的文本表示方法

1. TF-IDF（Term Frequency-Inverse Document Frequency）
TF-IDF 是一种统计方法，用于评估一个词在文档集合或语料库中的重要性。
TF（Term Frequency）：词频，表示一个词在文档中出现的频率。
IDF（Inverse Document Frequency）：逆文档频率，表示一个词在整个语料库中的稀有程度。

2. CountVectorizer
CountVectorizer 是一种词袋模型（Bag of Words, BoW）的实现，用于将文本转换为词频向量。
它会统计每个词在文档中出现的次数，并生成一个稀疏矩阵表示。

3. Embedding（词嵌入）
Embedding 是一种将词或句子映射到低维稠密向量的技术。
常见的词嵌入方法包括 Word2Vec、GloVe、FastText 和 BERT 等。
这些方法能够捕捉词与词之间的语义关系，如同义词、反义词等。
```

| **特性**       | **TF-IDF**         | **CountVectorizer** | **Embedding**         |
| -------------- | ------------------ | ------------------- | --------------------- |
| **表示方式**   | 稀疏向量           | 稀疏向量            | 稠密向量              |
| **语义信息**   | 无                 | 无                  | 有                    |
| **词序信息**   | 无                 | 无                  | 部分模型（如 BERT）有 |
| **维度**       | 高（词汇表大小）   | 高（词汇表大小）    | 低（通常 100-300 维） |
| **计算复杂度** | 低                 | 低                  | 高                    |
| **适用场景**   | 文本分类、信息检索 | 简单文本分类、聚类  | 复杂 NLP 任务         |

```
如何选择？
如果任务简单且计算资源有限：

选择 TF-IDF 或 CountVectorizer。
例如，文本分类、关键词提取等任务。

如果需要捕捉语义信息：
选择 Embedding。
例如，情感分析、机器翻译等任务。
如果需要处理大规模数据集或复杂任务：
选择预训练的 Embedding 模型（如 BERT）。
这些模型能够提供更好的语义表示和泛化能力。
```

```
示例
假设我们有一个简单的句子：“我喜欢自然语言处理”。

TF-IDF：
将句子转换为向量，表示每个词的重要性。
例如，[0.5, 0.3, 0.8]（假设词汇表为[“我”, “喜欢”, “自然语言处理”]）。

CountVectorizer：
将句子转换为词频向量。
例如，[1, 1, 1]（表示每个词出现一次）。

Embedding：
将句子中的每个词转换为稠密向量。
例如，使用 Word2Vec，“我”可能表示为 [0.1, 0.2, 0.3]，“喜欢”表示为 [0.4, 0.5, 0.6]，然后取平均或拼接得到句子向量。
```

### **TF-IDF、CountVectorizer 和 Embedding的区别**

- TF-IDF 和 CountVectorizer 适用于简单任务，计算效率高，但无法捕捉语义信息。
- Embedding 适用于复杂任务，能够捕捉语义信息，但需要更多的计算资源和数据。
- 在实际应用中，可以根据任务需求、计算资源和数据规模选择合适的文本表示方法。



## 3.2 5种不同的分类器

- RidgeClassifier岭回归
- RNN
- CNN
- LSTM
- Bert

```
1. RidgeClassifier（岭回归分类器）

概念
RidgeClassifier 是一种线性分类器，基于岭回归（Ridge Regression）的思想。
岭回归是一种线性回归的变体，通过引入 L2 正则化项来防止过拟合。

优点：简单高效，计算速度快。适用于高维数据，能够处理多重共线性。
缺点：只能处理线性可分或近似线性可分的问题。无法捕捉非线性关系。

应用场景:适用于特征数量较多且数据线性可分或近似线性可分的分类任务。

2. RNN（循环神经网络）
概念
RNN 是一种用于处理序列数据的神经网络，能够捕捉序列中的时间依赖性。
它通过循环结构将前一个时间步的隐藏状态传递到下一个时间步。

优点：能够处理变长序列数据。适用于自然语言处理、时间序列预测等任务。
缺点：容易出现梯度消失或梯度爆炸问题，导致长序列训练困难。计算效率较低，难以并行化。

应用场景: 自然语言处理（如文本生成、机器翻译）、时间序列分析、语音识别等。

3. CNN（卷积神经网络）
概念
CNN 是一种专门用于处理网格结构数据（如图像、音频）的神经网络。
它通过卷积层、池化层和全连接层提取数据的局部特征。

优点：能够自动提取数据的局部特征，减少人工特征工程。对平移、缩放、旋转等变换具有一定的鲁棒性。
缺点：主要适用于网格结构数据，对序列数据的处理能力有限。池化操作可能导致信息丢失。

应用场景: 图像处理（如图像分类、目标检测）、音频处理、视频分析等。


4. LSTM（长短期记忆网络）
概念
LSTM 是一种特殊的 RNN，通过引入门控机制（输入门、遗忘门、输出门）来解决梯度消失或梯度爆炸问题。
它能够更好地捕捉长序列中的长期依赖关系。

优点：能够处理长序列数据，避免梯度消失或梯度爆炸。适用于需要长期记忆的任务。
缺点：结构复杂，训练时间较长。

应用场景: 自然语言处理（如文本生成、情感分析）、时间序列预测、语音识别等。

5. BERT（Bidirectional Encoder Representations from Transformers）
概念
BERT 是一种基于 Transformer 架构的预训练语言模型。
它通过双向编码器学习文本的上下文表示，能够捕捉词与词之间的复杂关系。

优点：能够生成高质量的上下文表示，适用于多种 NLP 任务。预训练模型可以在大规模数据上学习通用语言知识，减少下游任务的训练时间和数据需求。
缺点：模型庞大，计算资源消耗巨大。需要大量的预训练数据和计算资源。

应用场景: 自然语言处理（如文本分类、命名实体识别、问答系统、机器翻译等）。
```

### **对比总结**

| **特性**       | **RidgeClassifier** | **RNN**                | **CNN**            | **LSTM**               | **BERT**                 |
| -------------- | ------------------- | ---------------------- | ------------------ | ---------------------- | ------------------------ |
| **类型**       | 线性分类器          | 序列模型               | 网格数据模型       | 序列模型               | 预训练语言模型           |
| **输入数据**   | 特征向量            | 序列数据               | 网格数据（如图像） | 序列数据               | 文本数据                 |
| **非线性能力** | 无                  | 有                     | 有                 | 有                     | 有                       |
| **长期依赖**   | 无                  | 有限                   | 无                 | 强                     | 强                       |
| **计算复杂度** | 低                  | 中等                   | 中等               | 高                     | 非常高                   |
| **应用场景**   | 线性分类任务        | 自然语言处理、时间序列 | 图像处理、音频处理 | 自然语言处理、时间序列 | 自然语言处理（多种任务） |



### **如何选择？**

- 如果数据线性可分或近似线性可分：
  - 选择 RidgeClassifier。
  - 例如，简单的二分类任务。
- 如果处理序列数据且需要捕捉时间依赖性：
  - 选择 RNN 或 LSTM。
  - 如果序列较长或需要长期记忆，优先选择 LSTM。
- 如果处理网格结构数据（如图像）：
  - 选择 CNN。
  - 例如，图像分类、目标检测等任务。
- 如果处理自然语言处理任务且需要高质量的上下文表示：
  - 选择 BERT 或其他预训练语言模型。
  - 例如，文本分类、命名实体识别、问答系统等任务。



### **示例**

- RidgeClassifier：
  - 适用于简单的文本分类任务，如垃圾邮件检测（假设特征已提取为向量）。
- RNN：
  - 适用于简单的文本生成任务，如字符级语言模型。
- CNN：
  - 适用于图像分类任务，如识别手写数字（MNIST 数据集）。
- LSTM：
  - 适用于情感分析任务，如分析电影评论的情感倾向。
- BERT：
  - 适用于复杂的自然语言处理任务，如问答系统、文本摘要等。



### **结论**

- RidgeClassifier 适用于线性分类任务，计算效率高。
- RNN 和 LSTM 适用于序列数据处理，LSTM 更适合长序列。
- CNN 适用于网格结构数据，如图像和音频。
- BERT 是一种强大的预训练语言模型，适用于多种自然语言处理任务。
- 在实际应用中，应根据任务需求、数据类型和计算资源选择合适的模型。



# 4. AI工程

## 4.1 CountVectorizer + RidgeClassifier

