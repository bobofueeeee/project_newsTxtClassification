# RidgeClassifier岭回归

**RidgeClassifier简介**

RidgeClassifier是一个基于岭回归（Ridge Regression）的分类器，它主要用于处理具有多重共线性特征的分类问题。该分类器通过将目标值转换为{-1, 1}（对于二进制分类）或采用多输出回归方案（对于多类别分类），将分类问题转化为回归任务。

**算法背景**

- 起源：岭回归由统计学家Hoerl和Kennard于1970年代初期提出，起初用于解决线性回归中的共线性问题。后来，该算法被扩展到分类任务，形成了RidgeClassifier。
- 应用领域：包括生物统计、金融分析、工程领域、社会科学等。例如，在基因数据分析中处理高维数据集，在投资风险评估中预测市场趋势，以及在系统设计和可靠性分析中进行优化。

**特点**

- 处理共线性：RidgeClassifier能够有效解决特征之间高度相关的问题，提高模型的稳定性。
- 灵活应用：适用于多种数据集，特别是在特征数量众多时表现出色。
- 局限性：尽管RidgeClassifier在处理共线性问题上具有优势，但在处理非线性数据方面可能存在一定的局限性。

# TF-IDF

项目地址：https://tianchi.aliyun.com/course/316/3596

#### **N-gram**

N-gram与Count Vectors类似，不过加入了相邻单词组合成为新的单词，并进行计数。

如果N取值为2，则句子1和句子2就变为：

```
句子1：我爱 爱北 北京 京天 天安 安门
句子2：我喜 喜欢 欢上 上海
```

#### **TF-IDF**

TF-IDF 分数由两部分组成：第一部分是**词语频率**（Term Frequency），第二部分是**逆文档频率**（Inverse Document Frequency）。其中计算语料库中文档总数除以含有该词语的文档数量，然后再取对数就是逆文档频率。

```
TF(t)= 该词语在当前文档出现的次数 / 当前文档中词语的总数
IDF(t)= log_e（文档总数 / 出现该词语的文档总数）
```

# 构建vocab +  Embedding（嵌入）

### 1. Vocab（词汇表）

**定义**：

- Vocab是一个包含所有已知单词或符号的列表。在训练模型时，文本数据中的每个单词或符号都会被映射到这个列表中的一个索引上。

**构建过程**：

1. **数据预处理**：首先，从文本数据集中收集所有独特的单词或符号。
2. **过滤**：根据频率或其他标准（如最低词频阈值）来过滤单词，只保留最常用的单词。
3. **排序和索引**：将剩余的单词按照某种顺序（如字母顺序或频率顺序）排序，并为每个单词分配一个唯一的索引。

**示例**：

假设我们有一个简单的文本数据集，其中包含以下单词：“I”, “like”, “apples”, “bananas”, “and”, “oranges”。经过预处理和过滤后，我们得到以下的vocab：

![image-20240612180022896](https://gitee.com/fubob/note-pic/raw/master/image/image-20240612180022896.png)

### 2. Embedding（嵌入）

**定义**：

- Embedding是一种将单词或其他文本元素（如短语、句子等）转换为固定大小向量的技术。这些向量通常被称为嵌入向量或词嵌入，它们可以捕捉单词之间的语义和语法关系。

**构建过程**：

1. **初始化**：首先，为vocab中的每个单词创建一个空的嵌入向量。这些向量的大小（即嵌入维度）是预先设定的。
2. **训练**：在模型训练过程中，使用文本数据来更新这些嵌入向量的值。具体的训练方法取决于所使用的模型（如Word2Vec、GloVe等）。

以Word2Vec的Skip-gram模型为例，该模型的目标是根据给定的中心词来预测其上下文词。在训练过程中，模型的参数（包括嵌入向量）会不断被更新，以最小化预测错误。

**示例**（使用上面的vocab）：

假设我们设定嵌入维度为3，那么每个单词的嵌入向量就是一个长度为3的浮点数数组。在初始化时，这些数组可能包含随机值。然后，在模型训练过程中，这些值会根据模型的目标函数（如交叉熵损失）进行更新。

例如，“apples”的初始嵌入向量可能是`[0.1, -0.2, 0.3]`，但经过训练后，这个向量可能会变为`[0.4, 0.1, -0.2]`，以更好地捕捉“apples”与其他单词之间的语义关系。

# Vocab到Embedding模型的过程

所以，总结来说，先把自然语言通过Vocab转换为数字，再由Embedding转换为向量，这样理解正确吗

是的，您的理解完全正确。在自然语言处理中，我们通常首先通过词汇表（Vocab）将自然语言文本中的单词或符号转换为数字索引（整数），这个过程通常被称为“分词”或“标记化”（tokenization）。然后，这些数字索引被用作嵌入层（Embedding layer）的输入，嵌入层将这些索引转换为固定大小的向量表示（即嵌入向量）。

这个过程的关键在于，嵌入向量能够捕捉到单词之间的语义和语法关系，这使得神经网络模型能够更好地理解文本数据。嵌入向量通常是通过在大规模语料库上训练得到的，例如使用Word2Vec、GloVe等算法。这些算法通过学习单词的上下文信息来生成嵌入向量，使得相似的单词在向量空间中的位置相近。

因此，通过Vocab将自然语言转换为数字索引，再通过Embedding将这些索引转换为向量表示，是自然语言处理中常见的预处理步骤，它们共同为后续的模型训练和应用提供了基础。